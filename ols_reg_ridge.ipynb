{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjgJ6iOJOn5W",
        "outputId": "7608e9a6-ccfc-430f-dd91-42c24715496f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== CV (OLS padronizado) === {'R2_CV': -6460503.018, 'MAE_CV': 2.865, 'RMSE_CV': 136.874}\n",
            "=== CV (Ridge) === {'R2_CV': -5060261.805, 'MAE_CV': 2.62, 'RMSE_CV': 125.188}\n",
            "\n",
            "=== In-sample (Ridge) === {'R2_in': 0.031, 'MAE_in': 0.082, 'RMSE_in': np.float64(0.111)}\n",
            "Resumo OLS (HC3) salvo.\n",
            "\n",
            "Arquivos salvos em: ./saidas_reg\n"
          ]
        }
      ],
      "source": [
        "import os, re, unicodedata, warnings, json\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV\n",
        "from sklearn.model_selection import GroupKFold, KFold\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "# ---------------- Parâmetros ----------------\n",
        "PATH_XLSX = \"base_logit_2001_2008.xlsx\"\n",
        "OUT_DIR   = \"./saidas_reg\"\n",
        "ROUND = 3\n",
        "N_SPLITS = 5\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---------------- Normalização de nomes ----------------\n",
        "def unaccent(s: str) -> str:\n",
        "    return \"\".join(ch for ch in unicodedata.normalize(\"NFKD\", str(s)) if not unicodedata.combining(ch))\n",
        "\n",
        "def norm_colname(s: str) -> str:\n",
        "    s = unaccent(s).lower()\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    s = s.replace(\"ebtida\", \"ebitda\")  # corrige variante comum\n",
        "    return re.sub(r\"[^a-z0-9]\", \"\", s)  # só [a-z0-9]\n",
        "\n",
        "def normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df2 = df.copy()\n",
        "    df2.columns = [norm_colname(c) for c in df.columns]\n",
        "    return df2\n",
        "\n",
        "def load_base_xlsx(path: str):\n",
        "    xl = pd.ExcelFile(path)\n",
        "    sheet = \"base\" if \"base\" in xl.sheet_names else max(xl.sheet_names, key=lambda s: xl.parse(s).shape[0])\n",
        "    raw = xl.parse(sheet, header=0)\n",
        "    norm = normalize_df(raw)\n",
        "    return raw, norm, sheet\n",
        "\n",
        "# ---------------- Localizadores ----------------\n",
        "def first_match(df_norm: pd.DataFrame, tokens: list[str]):\n",
        "    for c in df_norm.columns:\n",
        "        if all(t in c for t in tokens):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def pick_ops_col(df_norm: pd.DataFrame):\n",
        "    for p in [\"ops\",\"idops\",\"codigoops\",\"codops\",\"cnpj\"]:\n",
        "        if p in df_norm.columns: return p\n",
        "    for c in df_norm.columns:\n",
        "        if re.search(r\"(?:^|[^a-z0-9])(ops|idops|codigoops|codops|cnpj)(?:$|[^a-z0-9])\", c):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "# Alvo (DADOS_CONTAB_AUSENTES) — padrões\n",
        "TARGET_PATTERNS = [\n",
        "    [\"dados\",\"contab\",\"ausent\"],\n",
        "    [\"falt\",\"contab\"],\n",
        "    [\"nulo\",\"contab\"]\n",
        "]\n",
        "\n",
        "def find_target_col(df_norm: pd.DataFrame) -> str:\n",
        "    for pats in TARGET_PATTERNS:\n",
        "        c = first_match(df_norm, pats)\n",
        "        if c: return c\n",
        "    raise ValueError(\"Não encontrei a coluna alvo 'dados_contab_ausentes' (ou variações) na planilha.\")\n",
        "\n",
        "# Preditores — mapeamento por tokens (lista fornecida)\n",
        "PRED_TOKENS = {\n",
        "    \"contrap_efetivas\":            [\"contrap\",\"efetiv\"],\n",
        "    \"resultado_bruto\":             [\"resultado\",\"bruto\"],\n",
        "    \"resultado_liquido\":           [\"resultado\",\"liquid\"],\n",
        "    \"ebit\":                        [\"ebit\"],            # evitar confundir com ebitda\n",
        "    \"ebitda\":                      [\"ebitda\"],\n",
        "    \"divida_liquida\":              [\"divida\",\"liquid\"],\n",
        "    \"resultado_financeiro_liquido\":[\"result\",\"finance\",\"liquid\"],\n",
        "    \"roa\":                         [\"roa\"],\n",
        "    \"roe\":                         [\"roe\"],\n",
        "    \"mlb\":                         [\"mlb\"],\n",
        "    \"mll\":                         [\"mll\"],\n",
        "    \"margem_ebit\":                 [\"margem\",\"ebit\"],\n",
        "    \"margem_ebitda\":               [\"margem\",\"ebitda\"],\n",
        "    \"imob\":                        [\"imob\"],\n",
        "    \"endiv\":                       [\"endiv\"],\n",
        "    \"ce\":                          [\"ce\"],\n",
        "    \"dm\":                          [\"dm\"],\n",
        "    \"dc\":                          [\"dc\"],\n",
        "    \"da\":                          [\"da\"],\n",
        "    \"comb\":                        [\"comb\"],\n",
        "    \"comba\":                       [\"comba\"],\n",
        "    \"pcmr\":                        [\"pcmr\"],\n",
        "    \"pmpe\":                        [\"pmpe\"],\n",
        "    \"lg\":                          [\"lg\"],\n",
        "    \"lc\":                          [\"lc\"],\n",
        "    \"divida_bruta_ativos\":         [\"divida\",\"brut\",\"ativo\"],\n",
        "    \"divida_liquida_ativos\":       [\"divida\",\"liquid\",\"ativo\"],\n",
        "    \"divida_liquida_ebit\":         [\"divida\",\"liquid\",\"ebit\"],\n",
        "    \"divida_liquida_ebitda\":       [\"divida\",\"liquid\",\"ebitda\"],\n",
        "    \"pl_ativos\":                   [\"pl\",\"ativo\"],\n",
        "    \"divida_bruta_pl\":             [\"divida\",\"brut\",\"pl\"],\n",
        "    \"divida_liquida_pl\":           [\"divida\",\"liquid\",\"pl\"],\n",
        "    \"gat\":                         [\"gat\"],\n",
        "}\n",
        "SHORT_EXACT = {\"roa\",\"roe\",\"mlb\",\"mll\",\"ce\",\"dm\",\"dc\",\"da\",\"comb\",\"comba\",\"pcmr\",\"pmpe\",\"lg\",\"lc\",\"gat\",\"ebit\",\"ebitda\",\"pl\"}\n",
        "\n",
        "def find_predictors(df_norm: pd.DataFrame):\n",
        "    mapping = {}\n",
        "    used = set()\n",
        "    cols = df_norm.columns.tolist()\n",
        "    for label, toks in PRED_TOKENS.items():\n",
        "        found = None\n",
        "        # match exato para siglas curtas\n",
        "        if len(toks)==1 and toks[0] in SHORT_EXACT:\n",
        "            if toks[0] in cols:\n",
        "                found = toks[0]\n",
        "            else:\n",
        "                for c in cols:\n",
        "                    if re.search(rf\"(?:^|[^a-z0-9]){re.escape(toks[0])}(?:$|[^a-z0-9])\", c):\n",
        "                        found = c; break\n",
        "            # proteger 'ebit' ≠ 'ebitda'\n",
        "            if toks[0]==\"ebit\" and found and \"ebitda\" in found:\n",
        "                found = None\n",
        "        # tokens (todos presentes)\n",
        "        if not found:\n",
        "            for c in cols:\n",
        "                if toks==[\"ebit\"] and \"ebitda\" in c:\n",
        "                    continue\n",
        "                if all(t in c for t in toks):\n",
        "                    found = c; break\n",
        "        mapping[label] = found if (found and found not in used) else None\n",
        "        if mapping[label]:\n",
        "            used.add(mapping[label])\n",
        "    found_cols = [c for c in mapping.values() if c]\n",
        "    return found_cols, mapping\n",
        "\n",
        "# ---------------- Avaliação em CV ----------------\n",
        "def avaliar_regressao_cv(pipe, X, y, grupos=None, n_splits=5):\n",
        "    if grupos is not None:\n",
        "        cv = GroupKFold(n_splits=min(n_splits, len(np.unique(grupos)))).split(X, y, grupos)\n",
        "    else:\n",
        "        cv = KFold(n_splits=n_splits, shuffle=True, random_state=42).split(X, y)\n",
        "    r2s, maes, rmses = [], [], []\n",
        "    for tr, te in cv:\n",
        "        pipe.fit(X.iloc[tr], y[tr])\n",
        "        p = pipe.predict(X.iloc[te])\n",
        "        r2s.append(r2_score(y[te], p))\n",
        "        maes.append(mean_absolute_error(y[te], p))\n",
        "        rmses.append(np.sqrt(mean_squared_error(y[te], p)))\n",
        "    return {\"R2_CV\": float(np.mean(r2s)),\n",
        "            \"MAE_CV\": float(np.mean(maes)),\n",
        "            \"RMSE_CV\": float(np.mean(rmses))}\n",
        "\n",
        "# ---------------- Carregar e preparar ----------------\n",
        "df_raw, df_norm, sheet = load_base_xlsx(PATH_XLSX)\n",
        "col_ops = pick_ops_col(df_norm)\n",
        "\n",
        "# Alvo\n",
        "col_y = find_target_col(df_norm)\n",
        "y = pd.to_numeric(df_norm[col_y], errors=\"coerce\").values\n",
        "\n",
        "# Preditores (SEM dummy_ano)\n",
        "pred_cols, pred_map = find_predictors(df_norm)\n",
        "if not pred_cols:\n",
        "    raise ValueError(\"Nenhum preditor da lista foi encontrado. Verifique os nomes na planilha.\")\n",
        "X = df_norm[pred_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# Grupos para CV (por OPS se existir)\n",
        "grupos = df_norm[col_ops].astype(str).values if col_ops else None\n",
        "\n",
        "# ---------------- Modelos ----------------\n",
        "preprocess = Pipeline(steps=[\n",
        "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "])\n",
        "\n",
        "model_ols = Pipeline(steps=[\n",
        "    (\"prep\", preprocess),\n",
        "    (\"ols\", LinearRegression())\n",
        "])\n",
        "\n",
        "model_ridge = Pipeline(steps=[\n",
        "    (\"prep\", preprocess),\n",
        "    (\"ridge\", RidgeCV(alphas=np.logspace(-3, 3, 13), store_cv_values=False))\n",
        "])\n",
        "\n",
        "# ---------------- Avaliação (CV) ----------------\n",
        "met_ols  = avaliar_regressao_cv(model_ols,  X, y, grupos=grupos, n_splits=N_SPLITS)\n",
        "met_rdg  = avaliar_regressao_cv(model_ridge, X, y, grupos=grupos, n_splits=N_SPLITS)\n",
        "\n",
        "print(\"\\n=== CV (OLS padronizado) ===\", {k: round(v, ROUND) for k,v in met_ols.items()})\n",
        "print(\"=== CV (Ridge) ===\",          {k: round(v, ROUND) for k,v in met_rdg.items()})\n",
        "\n",
        "# ---------------- Ajuste final (Ridge) ----------------\n",
        "model_ridge.fit(X, y)\n",
        "yhat = model_ridge.predict(X)\n",
        "\n",
        "# In-sample\n",
        "r2_in   = r2_score(y, yhat)\n",
        "mae_in  = mean_absolute_error(y, yhat)\n",
        "rmse_in = np.sqrt(mean_squared_error(y, yhat))\n",
        "print(\"\\n=== In-sample (Ridge) ===\", {\"R2_in\": round(r2_in, ROUND),\n",
        "                                      \"MAE_in\": round(mae_in, ROUND),\n",
        "                                      \"RMSE_in\": round(rmse_in, ROUND)})\n",
        "\n",
        "# Coeficientes padronizados (efeito por 1 desvio-padrão)\n",
        "prep = model_ridge.named_steps[\"prep\"]\n",
        "X_std = prep.fit_transform(X)\n",
        "ridge = model_ridge.named_steps[\"ridge\"].fit(X_std, y)\n",
        "coef_ridge = ridge.coef_\n",
        "feat_names = X.columns.tolist()\n",
        "coef_ridge_df = pd.DataFrame({\"feature\": feat_names, \"coef_std\": coef_ridge}).sort_values(\"coef_std\", key=np.abs, ascending=False)\n",
        "coef_ridge_df[\"coef_std\"] = coef_ridge_df[\"coef_std\"].round(ROUND)\n",
        "\n",
        "# Também OLS padronizado (coeficientes)\n",
        "model_ols.fit(X, y)\n",
        "ols = model_ols.named_steps[\"ols\"]\n",
        "coef_ols = ols.coef_\n",
        "coef_ols_df = pd.DataFrame({\"feature\": feat_names, \"coef_std\": coef_ols}).sort_values(\"coef_std\", key=np.abs, ascending=False)\n",
        "coef_ols_df[\"coef_std\"] = coef_ols_df[\"coef_std\"].round(ROUND)\n",
        "\n",
        "# Predições\n",
        "preds = pd.DataFrame({\"y_true\": np.round(y, ROUND), \"y_hat\": np.round(yhat, ROUND)})\n",
        "if col_ops: preds[\"ops\"] = df_norm[col_ops].astype(str).values\n",
        "\n",
        "# ---------------- Salvar ----------------\n",
        "coef_ridge_df.to_csv(os.path.join(OUT_DIR, \"coeficientes_padronizados_ridge.csv\"), index=False, encoding=\"utf-8\")\n",
        "coef_ols_df.to_csv(os.path.join(OUT_DIR, \"coeficientes_padronizados_ols.csv\"), index=False, encoding=\"utf-8\")\n",
        "\n",
        "pd.DataFrame([{\n",
        "    **{f\"ols_{k}\": round(v, ROUND) for k,v in met_ols.items()},\n",
        "    **{f\"ridge_{k}\": round(v, ROUND) for k,v in met_rdg.items()},\n",
        "    \"R2_in\": round(r2_in, ROUND),\n",
        "    \"MAE_in\": round(mae_in, ROUND),\n",
        "    \"RMSE_in\": round(rmse_in, ROUND),\n",
        "    \"n_preditores\": int(X.shape[1]),\n",
        "    \"n_obs\": int(len(y)),\n",
        "    \"cv_por_ops\": bool(col_ops)\n",
        "}]).to_csv(os.path.join(OUT_DIR, \"metricas.csv\"), index=False, encoding=\"utf-8\")\n",
        "\n",
        "preds.to_csv(os.path.join(OUT_DIR, \"predicoes_in_sample_ridge.csv\"), index=False, encoding=\"utf-8\")\n",
        "\n",
        "# Mapeamento indicador -> coluna encontrada\n",
        "mapping = []\n",
        "for label, toks in PRED_TOKENS.items():\n",
        "    col_found = None\n",
        "    if len(toks)==1 and toks[0] in SHORT_EXACT and toks[0] in df_norm.columns:\n",
        "        col_found = toks[0]\n",
        "        if toks[0]==\"ebit\" and \"ebitda\" in col_found:\n",
        "            col_found = None\n",
        "    if not col_found:\n",
        "        for c in df_norm.columns:\n",
        "            if toks==[\"ebit\"] and \"ebitda\" in c:\n",
        "                continue\n",
        "            if all(t in c for t in toks):\n",
        "                col_found = c; break\n",
        "    mapping.append({\"indicador_canonico\": label, \"coluna_encontrada_norm\": col_found or \"(não encontrado)\"})\n",
        "pd.DataFrame(mapping).to_csv(os.path.join(OUT_DIR, \"mapeamento_colunas.csv\"), index=False, encoding=\"utf-8\")\n",
        "\n",
        "# (Opcional) OLS com p-valores (statsmodels, erros robustos HC3)\n",
        "try:\n",
        "    import statsmodels.api as sm\n",
        "    X_imp = pd.DataFrame(SimpleImputer(strategy=\"median\").fit_transform(X), columns=X.columns, index=X.index)\n",
        "    X_std_sm = (X_imp - X_imp.mean()) / X_imp.std(ddof=0)\n",
        "    X_sm = sm.add_constant(X_std_sm)\n",
        "    ols_sm = sm.OLS(y, X_sm).fit(cov_type=\"HC3\")\n",
        "    with open(os.path.join(OUT_DIR, \"ols_resumo.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(str(ols_sm.summary()))\n",
        "    tbl = pd.DataFrame({\"coef_std\": ols_sm.params, \"std_err\": ols_sm.bse, \"t\": ols_sm.tvalues, \"pval\": ols_sm.pvalues}).round(ROUND)\n",
        "    tbl.to_csv(os.path.join(OUT_DIR, \"ols_coeficientes_pvalues.csv\"), encoding=\"utf-8\")\n",
        "    print(\"Resumo OLS (HC3) salvo.\")\n",
        "except Exception as e:\n",
        "    print(\"[Aviso] Statsmodels indisponível/erro; p-valores não gerados:\", e)\n",
        "\n",
        "print(\"\\nArquivos salvos em:\", OUT_DIR)\n"
      ]
    }
  ]
}